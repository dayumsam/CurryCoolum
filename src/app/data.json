{
    "summary": [
      {
        "heading": "What is HTTP?",
        "text": [
          "HTTP is a TCP/IP-based application layer communication protocol that standardizes how clients and servers communicate with each other.",
          "It defines how content is requested and transmitted across the internet.",
          "By application layer protocol, I mean that it's simply an abstraction layer that standardizes how hosts (clients and servers) communicate.",
          "HTTP itself depends on TCP/IP to get requests and responses between the client and server.",
          "By default, TCP port 80 is used, but other ports can also be used."
        ]
      },
      {
        "heading": "HTTP/0.9 - The One Liner (1991)",
        "text": [
          "The first documented version of HTTP was HTTP/0.9 which was put forward in 1991.",
          "It was the simplest protocol ever; having a single method called GET.",
          "If a client had to access some webpage on the server, it would have made the simple request like belowGET /index.htmlAnd the response from server would have looked as follows(response body)(connection closed)That is, the server would get the request, reply with the HTML in response and as soon as the content has been transferred, the connection will be closed.",
          "There wereNo headersGET was the only allowed methodResponse had to be HTMLAs you can see, the protocol really had nothing more than being a stepping stone for what was to come."
        ]
      },
      {
        "heading": "HTTP/1.0 - 1996",
        "text": [
          "In 1996, the next version of HTTP i.e. HTTP/1.0 evolved that vastly improved over the original version.",
          "Unlike HTTP/0.9 which was only designed for HTML response, HTTP/1.0 could now deal with other response formats i.e. images, video files, plain text or any other content type as well.",
          "It added more methods (i.e. POST and HEAD), request/response formats got changed, HTTP headers got added to both the request and responses, status codes were added to identify the response, character set support was introduced, multi-part types, authorization, caching, content encoding and more was included.",
          "Here is how a sample HTTP/1.0 request and response might have looked like:GET / HTTP/1.0Host: cs.fyiUser-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5)Accept: */*As you can see, alongside the request, client has also sent it's personal information, required response type etc.",
          "While in HTTP/0.9 client could never send such information because there were no headers.",
          "So, now that server could send any content type to the client; not so long after the introduction, the term \"Hyper Text\" in HTTP became misnomer.",
          "HMTP or Hypermedia transfer protocol might have made more sense but, I guess, we are stuck with the name for life.",
          "One of the major drawbacks of HTTP/1.0 were you couldn't have multiple requests per connection.",
          "That is, whenever a client will need something from the server, it will have to open a new TCP connection and after that single request has been fulfilled, connection will be closed.",
          "And for any next requirement, it will have to be on a new connection.",
          "Well, let's assume that you visit a webpage having 10 images, 5 stylesheets and 5 javascript files, totalling to 20 items that needs to fetched when request to that webpage is made.",
          "Since the server closes the connection as soon as the request has been fulfilled, there will be a series of 20 separate connections where each of the items will be served one by one on their separate connections.",
          "This large number of connections results in a serious performance hit as requiring a new TCP connection imposes a significant performance penalty because of three-way handshake followed by slow-start."
        ]
      },
      {
        "heading": "Three-way Handshake",
        "text": [
          "Three-way handshake in it's simples form is that all the TCP connections begin with a three-way handshake in which the client and the server share a series of packets before starting to share the application data.",
          "SYN - Client picks up a random number, let's say x, and sends it to the server.",
          "SYN ACK - Server acknowledges the request by sending an ACK packet back to the client which is made up of a random number, let's say y picked up by server and the number x+1 where x is the number that was sent by the clientACK - Client increments the number y received from the server and sends an ACK packet back with the number y+1Once the three-way handshake is completed, the data sharing between the client and server may begin.",
          "It should be noted that the client may start sending the application data as soon as it dispatches the last ACK packet but the server will still have to wait for the ACK packet to be recieved in order to fulfill the request.",
          "However, some implementations of HTTP/1.0 tried to overcome this issue by introducing a new header called Connection: keep-alive which was meant to tell the server \"Hey server, do not close this connection, I need it again\".",
          "But still, it wasn't that widely supported and the problem still persisted.",
          "Apart from being connectionless, HTTP also is a stateless protocol i.e. server doesn't maintain the information about the client and so each of the requests has to have the information necessary for the server to fulfill the request on it's own without any association with any old requests.",
          "And so this adds fuel to the fire i.e. apart from the large number of connections that the client has to open, it also has to send some redundant data on the wire causing increased bandwidth usage."
        ]
      },
      {
        "heading": "HTTP/1.1 - 1997",
        "text": [
          "After merely 3 years of HTTP/1.0, the next version i.e. HTTP/1.1 was released in 1999; which made alot of improvements over it's predecessor.",
          "The major improvements over HTTP/1.0 includedNew HTTP methods were added, which introduced PUT, PATCH, OPTIONS, DELETEHostname Identification In HTTP/1.0 Host header wasn't required but HTTP/1.1 made it required.",
          "Persistent Connections As discussed above, in HTTP/1.0 there was only one request per connection and the connection was closed as soon as the request was fulfilled which resulted in accute performance hit and latency problems.",
          "HTTP/1.1 introduced the persistent connections i.e. connections weren't closed by default and were kept open which allowed multiple sequential requests.",
          "To close the connections, the header Connection: close had to be available on the request.",
          "Clients usually send this header in the last request to safely close the connection.",
          "Pipelining It also introduced the support for pipelining, where the client could send multiple requests to the server without waiting for the response from server on the same connection and server had to send the response in the same sequence in which requests were received.",
          "But how does the client know that this is the point where first response download completes and the content for next response starts, you may ask!",
          "Well, to solve this, there must be Content-Length header present which clients can use to identify where the response ends and it can start waiting for the next response.",
          "It should be noted that in order to benefit from persistent connections or pipelining, Content-Length header must be available on the response, because this would let the client know when the transmission completes and it can send the next request (in normal sequential way of sending requests) or start waiting for the the next response (when pipelining is enabled).But there was still an issue with this approach.",
          "And that is, what if the data is dynamic and server cannot find the content length before hand?",
          "Well in that case, you really can't benefit from persistent connections, could you?!",
          "In order to solve this HTTP/1.1 introduced chunked encoding.",
          "In such cases server may omit content-Length in favor of chunked encoding (more to it in a moment).",
          "However, if none of them are available, then the connection must be closed at the end of request.",
          "Chunked Transfers In case of dynamic content, when the server cannot really find out the Content-Length when the transmission starts, it may start sending the content in pieces (chunk by chunk) and add the Content-Length for each chunk when it is sent.",
          "And when all of the chunks are sent i.e. whole transmission has completed, it sends an empty chunk i.e. the one with Content-Length set to zero in order to identify the client that transmission has completed.",
          "In order to notify the client about the chunked transfer, server includes the header Transfer-Encoding: chunkedUnlike HTTP/1.0 which had Basic authentication only, HTTP/1.1 included digest and proxy authenticationCachingByte RangesCharacter setsLanguage negotiationClient cookiesEnhanced compression supportNew status codes..and moreI am not going to dwell about all the HTTP/1.1 features in this post as it is a topic in itself and you can already find a lot about it.",
          "The one such document that I would recommend you to read is Key differences between HTTP/1.0 and HTTP/1.1 and here is the link to original RFC for the overachievers.",
          "HTTP/1.1 was introduced in 1999 and it had been a standard for many years.",
          "Although, it improved alot over it's predecessor; with the web changing everyday, it started to show it's age.",
          "Loading a web page these days is more resource-intensive than it ever was.",
          "A simple webpage these days has to open more than 30 connections.",
          "Well HTTP/1.1 has persistent connections, then why so many connections?",
          "The reason is, in HTTP/1.1 it can only have one outstanding connection at any moment of time.",
          "HTTP/1.1 tried to fix this by introducing pipelining but it didn't completely address the issue because of the head-of-line blocking where a slow or heavy request may block the requests behind and once a request gets stuck in a pipeline, it will have to wait for the next requests to be fulfilled.",
          "To overcome these shortcomings of HTTP/1.1, the developers started implementing the workarounds, for example use of spritesheets, encoded images in CSS, single humungous CSS/Javascript files, domain sharding etc."
        ]
      },
      {
        "heading": "SPDY - 2009",
        "text": [
          "Google went ahead and started experimenting with alternative protocols to make the web faster and improving web security while reducing the latency of web pages.",
          "In 2009, they announced SPDY.SPDY is a trademark of Google and isn't an acronym.",
          "It was seen that if we keep increasing the bandwidth, the network performance increases in the beginning but a point comes when there is not much of a performance gain.",
          "But if you do the same with latency i.e. if we keep dropping the latency, there is a constant performance gain.",
          "This was the core idea for performance gain behind SPDY, decrease the latency to increase the network performance.",
          "For those who don't know the difference, latency is the delay i.e. how long it takes for data to travel between the source and destination (measured in milliseconds) and bandwidth is the amount of data transfered per second (bits per second).The features of SPDY included, multiplexing, compression, prioritization, security etc.",
          "I am not going to get into the details of SPDY, as you will get the idea when we get into the nitty gritty of HTTP/2 in the next section as I said HTTP/2 is mostly inspired from SPDY.SPDY didn't really try to replace HTTP; it was a translation layer over HTTP which existed at the application layer and modified the request before sending it over to the wire.",
          "It started to become a defacto standards and majority of browsers started implementing it.",
          "In 2015, at Google, they didn't want to have two competing standards and so they decided to merge it into HTTP while giving birth to HTTP/2 and deprecating SPDY."
        ]
      },
      {
        "heading": "HTTP/2 - 2015",
        "text": [
          "By now, you must be convinced that why we needed another revision of the HTTP protocol.",
          "HTTP/2 was designed for low latency transport of content.",
          "The key features or differences from the old version of HTTP/1.1 includeBinary instead of TextualMultiplexing - Multiple asynchronous HTTP requests over a single connectionHeader compression using HPACKServer Push - Multiple responses for single requestRequest PrioritizationSecurity"
        ]
      },
      {
        "heading": "1. Binary Protocol",
        "text": [
          "HTTP/2 tends to address the issue of increased latency that existed in HTTP/1.x by making it a binary protocol.",
          "Being a binary protocol, it easier to parse but unlike HTTP/1.x it is no longer readable by the human eye.",
          "The major building blocks of HTTP/2 are Frames and Streams"
        ]
      },
      {
        "heading": "Frames and Streams",
        "text": [
          "HTTP messages are now composed of one or more frames.",
          "There is a HEADERS frame for the meta data and DATA frame for the payload and there exist several other types of frames (HEADERS, DATA, RST_STREAM, SETTINGS, PRIORITY etc) that you can check through the HTTP/2 specs.",
          "Every HTTP/2 request and response is given a unique stream ID and it is divided into frames.",
          "Frames are nothing but binary pieces of data.",
          "A collection of frames is called a Stream.",
          "Each frame has a stream id that identifies the stream to which it belongs and each frame has a common header.",
          "Also, apart from stream ID being unique, it is worth mentioning that, any request initiated by client uses odd numbers and the response from server has even numbers stream IDs.",
          "Apart from the HEADERS and DATA, another frame type that I think worth mentioning here is RST_STREAM which is a special frame type that is used to abort some stream i.e. client may send this frame to let the server know that I don't need this stream anymore.",
          "In HTTP/1.1 the only way to make the server stop sending the response to client was closing the connection which resulted in increased latency because a new connection had to be opened for any consecutive requests.",
          "While in HTTP/2, client can use RST_STREAM and stop receiving a specific stream while the connection will still be open and the other streams will still be in play."
        ]
      },
      {
        "heading": "2. Multiplexing",
        "text": [
          "Since HTTP/2 is now a binary protocol and as I said above that it uses frames and streams for requests and responses, once a TCP connection is opened, all the streams are sent asynchronously through the same connection without opening any additional connections.",
          "And in turn, the server responds in the same asynchronous way i.e. the response has no order and the client uses the assigned stream id to identify the stream to which a specific packet belongs.",
          "This also solves the head-of-line blocking issue that existed in HTTP/1.x i.e. the client will not have to wait for the request that is taking time and other requests will still be getting processed."
        ]
      },
      {
        "heading": "3. Header Compression",
        "text": [
          "It was part of a separate RFC which was specifically aimed at optimizing the sent headers.",
          "The essence of it is that when we are constantly accessing the server from a same client there is alot of redundant data that we are sending in the headers over and over, and sometimes there might be cookies increasing the headers size which results in bandwidth usage and increased latency.",
          "To overcome this, HTTP/2 introduced header compression.",
          "Unlike request and response, headers are not compressed in gzip or compress etc formats but there is a different mechanism in place for header compression which is literal values are encoded using Huffman code and a headers table is maintained by the client and server and both the client and server omit any repetitive headers (e.g. user agent etc) in the subsequent requests and reference them using the headers table maintained by both.",
          "While we are talking headers, let me add here that the headers are still the same as in HTTP/1.1, except for the addition of some pseudo headers i.e. :method, :scheme, :host and :path"
        ]
      },
      {
        "heading": "4. Server Push",
        "text": [
          "Server push is another tremendous feature of HTTP/2 where the server, knowing that the client is going to ask for a certain resource, can push it to the client without even client asking for it.",
          "For example, let's say a browser loads a web page, it parses the whole page to find out the remote content that it has to load from the server and then sends consequent requests to the server to get that content.",
          "Server push allows the server to decrease the roundtrips by pushing the data that it knows that client is going to demand.",
          "How it is done is, server sends a special frame called PUSH_PROMISE notifying the client that, \"Hey, I am about to send this resource to you!",
          "The PUSH_PROMISE frame is associated with the stream that caused the push to happen and it contains the promised stream ID i.e. the stream on which the server will send the resource to be pushed."
        ]
      },
      {
        "heading": "5. Request Prioritization",
        "text": [
          "A client can assign a priority to a stream by including the prioritization information in the HEADERS frame by which a stream is opened.",
          "At any other time, client can send a PRIORITY frame to change the priority of a stream.",
          "Without any priority information, server processes the requests asynchronously i.e. without any order.",
          "If there is priority assigned to a stream, then based on this prioritization information, server decides how much of the resources need to be given to process which request."
        ]
      },
      {
        "heading": "6. Security",
        "text": [
          "There was extensive discussion on whether security (through TLS) should be made mandatory for HTTP/2 or not.",
          "In the end, it was decided not to make it mandatory.",
          "However, most vendors stated that they will only support HTTP/2 when it is used over TLS.",
          "So, although HTTP/2 doesn't require encryption by specs but it has kind of become mandatory by default anyway.",
          "With that out of the way, HTTP/2 when implemented over TLS does impose some requirementsi.e.",
          "TLS version 1.2 or higher must be used, there must be a certain level of minimum keysizes, ephemeral keys are required etc."
        ]
      },
      {
        "heading": "HTTP/3 - 2022",
        "text": [
          "QUIC is a transport layer protocol which is built on top of UDP and is designed to be a replacement for TCP.",
          "It is a multiplexed, secure, stream-based protocol which is designed to reduce latency and improve performance.",
          "It is a successor to TCP and HTTP/2.QUIC is a multiplexed, secure, stream-based protocol which is designed to reduce latency and improve performance."
        ]
      },
      {
        "heading": "1. Multiplexing",
        "text": [
          "QUIC is a multiplexed protocol which means that multiple streams can be sent over a single connection.",
          "This is similar to HTTP/2 where multiple streams can be sent over a single connection.",
          "However, unlike HTTP/2, QUIC is not limited to HTTP.",
          "It can be used for any application that requires reliable, ordered, and loss-tolerant delivery of streams of data."
        ]
      },
      {
        "heading": "2. Stream-based",
        "text": [
          "QUIC is a stream-based protocol which means that data is sent in the form of streams.",
          "Each stream is identified by a unique stream ID.",
          "QUIC uses a single stream to send data in both directions.",
          "This is similar to HTTP/2 where each stream is identified by a unique stream ID and each stream is bi-directional."
        ]
      },
      {
        "heading": "3. Unreliable Datagram",
        "text": [
          "QUIC uses unreliable datagrams to send data.",
          "This means that QUIC does not guarantee that the data will be delivered to the receiver.",
          "However, QUIC does guarantee that the data will be delivered in the same order in which it was sent.",
          "This is similar to UDP where data is sent in the form of datagrams and the datagrams are not guaranteed to be delivered to the receiver."
        ]
      },
      {
        "heading": "4. Connection Migration",
        "text": [
          "QUIC supports connection migration which means that a QUIC connection can be migrated from one IP address to another IP address.",
          "This is similar to TCP where a TCP connection can be migrated from one IP address to another IP address."
        ]
      },
      {
        "heading": "5. Loss Recovery",
        "text": [
          "QUIC uses loss recovery to recover from packet loss.",
          "QUIC uses a combination of congestion control and loss recovery to recover from packet loss.",
          "This is similar to TCP where TCP uses a combination of congestion control and loss recovery to recover from packet loss."
        ]
      },
      {
        "heading": "6. Congestion Control",
        "text": [
          "QUIC uses congestion control to control the rate at which data is sent over the network.",
          "QUIC uses a combination of congestion control and loss recovery to recover from packet loss.",
          "This is similar to TCP where TCP uses a combination of congestion control and loss recovery to recover from packet loss."
        ]
      },
      {
        "heading": "7. Handshake",
        "text": [
          "QUIC uses a handshake to establish a secure connection between the client and the server.",
          "QUIC uses TLS 1.3 to establish a secure connection between the client and the server.",
          "This is similar to HTTP/2 where TLS 1.2 is used to establish a secure connection between the client and the server."
        ]
      },
      {
        "heading": "8. Header Compression",
        "text": [
          "QUIC uses header compression to reduce the size of the headers.",
          "This is similar to HTTP/2 where HPACK is used to compress the headers."
        ]
      },
      {
        "heading": "9. Security",
        "text": [
          "QUIC uses TLS 1.3 to establish a secure connection between the client and the server.",
          "This is similar to HTTP/2 where TLS 1.2 is used to establish a secure connection between the client and the server."
        ]
      },
      {
        "heading": "",
        "text": [
          "In this article, we have discussed HTTP/1.1, HTTP/2, and HTTP/3.",
          "We have also discussed the differences between HTTP/1.1 and HTTP/2 and HTTP/2 and HTTP/3.",
          "If you have any questions, please feel free to reach out to me."
        ]
      }
    ],
    "article_text": "HTTP is the protocol that every web developer should know, as it powers the entire web. Knowing HTTP can certainly help you develop better applications.\n\nIn this article, I will discuss what HTTP is, how it came to be, where it stands today, and how we got here\n\nWhat is HTTP?\n\nFirst things first, what is HTTP? HTTP is a TCP/IP-based application layer communication protocol that standardizes how clients and servers communicate with each other. It defines how content is requested and transmitted across the internet. By application layer protocol, I mean that it's simply an abstraction layer that standardizes how hosts (clients and servers) communicate. HTTP itself depends on TCP/IP to get requests and responses between the client and server. By default, TCP port 80 is used, but other ports can also be used. HTTPS, however, uses port 443.\n\nHTTP/0.9 - The One Liner (1991)\n\nThe first documented version of HTTP was HTTP/0.9 which was put forward in 1991. It was the simplest protocol ever; having a single method called GET. If a client had to access some webpage on the server, it would have made the simple request like below\n\nGET /index.html\n\nAnd the response from server would have looked as follows\n\n(response body) (connection closed)\n\nThat is, the server would get the request, reply with the HTML in response and as soon as the content has been transferred, the connection will be closed. There were\n\nNo headers\n\nGET was the only allowed method\n\nwas the only allowed method Response had to be HTML\n\nAs you can see, the protocol really had nothing more than being a stepping stone for what was to come.\n\nHTTP/1.0 - 1996\n\nIn 1996, the next version of HTTP i.e. HTTP/1.0 evolved that vastly improved over the original version.\n\nUnlike HTTP/0.9 which was only designed for HTML response, HTTP/1.0 could now deal with other response formats i.e. images, video files, plain text or any other content type as well. It added more methods (i.e. POST and HEAD), request/response formats got changed, HTTP headers got added to both the request and responses, status codes were added to identify the response, character set support was introduced, multi-part types, authorization, caching, content encoding and more was included.\n\nHere is how a sample HTTP/1.0 request and response might have looked like:\n\nGET / HTTP/1.0 Host: cs.fyi User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) Accept: */*\n\nAs you can see, alongside the request, client has also sent it's personal information, required response type etc. While in HTTP/0.9 client could never send such information because there were no headers.\n\nExample response to the request above may have looked like below\n\nHTTP/1.0 200 OK Content-Type : text/plain Content-Length : 137582 Expires : Thu, 05 Dec 1997 16:00:00 GMT Last-Modified : Wed, 5 August 1996 15:55:28 GMT Server : Apache 0.84 (response body) (connection closed)\n\nIn the very beginning of the response there is HTTP/1.0 (HTTP followed by the version number), then there is the status code 200 followed by the reason phrase (or description of the status code, if you will).\n\nIn this newer version, request and response headers were still kept as ASCII encoded, but the response body could have been of any type i.e. image, video, HTML, plain text or any other content type. So, now that server could send any content type to the client; not so long after the introduction, the term \"Hyper Text\" in HTTP became misnomer. HMTP or Hypermedia transfer protocol might have made more sense but, I guess, we are stuck with the name for life.\n\nOne of the major drawbacks of HTTP/1.0 were you couldn't have multiple requests per connection. That is, whenever a client will need something from the server, it will have to open a new TCP connection and after that single request has been fulfilled, connection will be closed. And for any next requirement, it will have to be on a new connection. Why is it bad? Well, let's assume that you visit a webpage having 10 images, 5 stylesheets and 5 javascript files, totalling to 20 items that needs to fetched when request to that webpage is made. Since the server closes the connection as soon as the request has been fulfilled, there will be a series of 20 separate connections where each of the items will be served one by one on their separate connections. This large number of connections results in a serious performance hit as requiring a new TCP connection imposes a significant performance penalty because of three-way handshake followed by slow-start.\n\nThree-way Handshake\n\nThree-way handshake in it's simples form is that all the TCP connections begin with a three-way handshake in which the client and the server share a series of packets before starting to share the application data.\n\nSYN - Client picks up a random number, let's say x, and sends it to the server.\n\nSYN ACK - Server acknowledges the request by sending an ACK packet back to the client which is made up of a random number, let's say y picked up by server and the number x+1 where x is the number that was sent by the client\n\nACK - Client increments the number y received from the server and sends an ACK packet back with the number y+1\n\nOnce the three-way handshake is completed, the data sharing between the client and server may begin. It should be noted that the client may start sending the application data as soon as it dispatches the last ACK packet but the server will still have to wait for the ACK packet to be recieved in order to fulfill the request.\n\nHowever, some implementations of HTTP/1.0 tried to overcome this issue by introducing a new header called Connection: keep-alive which was meant to tell the server \"Hey server, do not close this connection, I need it again\". But still, it wasn't that widely supported and the problem still persisted.\n\nApart from being connectionless, HTTP also is a stateless protocol i.e. server doesn't maintain the information about the client and so each of the requests has to have the information necessary for the server to fulfill the request on it's own without any association with any old requests. And so this adds fuel to the fire i.e. apart from the large number of connections that the client has to open, it also has to send some redundant data on the wire causing increased bandwidth usage.\n\nHTTP/1.1 - 1997\n\nAfter merely 3 years of HTTP/1.0, the next version i.e. HTTP/1.1 was released in 1999; which made alot of improvements over it's predecessor. The major improvements over HTTP/1.0 included\n\nNew HTTP methods were added, which introduced PUT, PATCH, OPTIONS, DELETE\n\nHostname Identification In HTTP/1.0 Host header wasn't required but HTTP/1.1 made it required.\n\nPersistent Connections As discussed above, in HTTP/1.0 there was only one request per connection and the connection was closed as soon as the request was fulfilled which resulted in accute performance hit and latency problems. HTTP/1.1 introduced the persistent connections i.e. connections weren't closed by default and were kept open which allowed multiple sequential requests. To close the connections, the header Connection: close had to be available on the request. Clients usually send this header in the last request to safely close the connection.\n\nPipelining It also introduced the support for pipelining, where the client could send multiple requests to the server without waiting for the response from server on the same connection and server had to send the response in the same sequence in which requests were received. But how does the client know that this is the point where first response download completes and the content for next response starts, you may ask! Well, to solve this, there must be Content-Length header present which clients can use to identify where the response ends and it can start waiting for the next response.\n\nIt should be noted that in order to benefit from persistent connections or pipelining, Content-Length header must be available on the response, because this would let the client know when the transmission completes and it can send the next request (in normal sequential way of sending requests) or start waiting for the the next response (when pipelining is enabled). But there was still an issue with this approach. And that is, what if the data is dynamic and server cannot find the content length before hand? Well in that case, you really can't benefit from persistent connections, could you?! In order to solve this HTTP/1.1 introduced chunked encoding. In such cases server may omit content-Length in favor of chunked encoding (more to it in a moment). However, if none of them are available, then the connection must be closed at the end of request.\n\nChunked Transfers In case of dynamic content, when the server cannot really find out the Content-Length when the transmission starts, it may start sending the content in pieces (chunk by chunk) and add the Content-Length for each chunk when it is sent. And when all of the chunks are sent i.e. whole transmission has completed, it sends an empty chunk i.e. the one with Content-Length set to zero in order to identify the client that transmission has completed. In order to notify the client about the chunked transfer, server includes the header Transfer-Encoding: chunked\n\nUnlike HTTP/1.0 which had Basic authentication only, HTTP/1.1 included digest and proxy authentication\n\nCaching\n\nByte Ranges\n\nCharacter sets\n\nLanguage negotiation\n\nClient cookies\n\nEnhanced compression support\n\nNew status codes\n\n..and more\n\nI am not going to dwell about all the HTTP/1.1 features in this post as it is a topic in itself and you can already find a lot about it. The one such document that I would recommend you to read is Key differences between HTTP/1.0 and HTTP/1.1 and here is the link to original RFC for the overachievers.\n\nHTTP/1.1 was introduced in 1999 and it had been a standard for many years. Although, it improved alot over it's predecessor; with the web changing everyday, it started to show it's age. Loading a web page these days is more resource-intensive than it ever was. A simple webpage these days has to open more than 30 connections. Well HTTP/1.1 has persistent connections, then why so many connections? you say! The reason is, in HTTP/1.1 it can only have one outstanding connection at any moment of time. HTTP/1.1 tried to fix this by introducing pipelining but it didn't completely address the issue because of the head-of-line blocking where a slow or heavy request may block the requests behind and once a request gets stuck in a pipeline, it will have to wait for the next requests to be fulfilled. To overcome these shortcomings of HTTP/1.1, the developers started implementing the workarounds, for example use of spritesheets, encoded images in CSS, single humungous CSS/Javascript files, domain sharding etc.\n\nSPDY - 2009\n\nGoogle went ahead and started experimenting with alternative protocols to make the web faster and improving web security while reducing the latency of web pages. In 2009, they announced SPDY.\n\nSPDY is a trademark of Google and isn't an acronym.\n\nIt was seen that if we keep increasing the bandwidth, the network performance increases in the beginning but a point comes when there is not much of a performance gain. But if you do the same with latency i.e. if we keep dropping the latency, there is a constant performance gain. This was the core idea for performance gain behind SPDY, decrease the latency to increase the network performance.\n\nFor those who don't know the difference, latency is the delay i.e. how long it takes for data to travel between the source and destination (measured in milliseconds) and bandwidth is the amount of data transfered per second (bits per second).\n\nThe features of SPDY included, multiplexing, compression, prioritization, security etc. I am not going to get into the details of SPDY, as you will get the idea when we get into the nitty gritty of HTTP/2 in the next section as I said HTTP/2 is mostly inspired from SPDY.\n\nSPDY didn't really try to replace HTTP; it was a translation layer over HTTP which existed at the application layer and modified the request before sending it over to the wire. It started to become a defacto standards and majority of browsers started implementing it.\n\nIn 2015, at Google, they didn't want to have two competing standards and so they decided to merge it into HTTP while giving birth to HTTP/2 and deprecating SPDY.\n\nHTTP/2 - 2015\n\nBy now, you must be convinced that why we needed another revision of the HTTP protocol. HTTP/2 was designed for low latency transport of content. The key features or differences from the old version of HTTP/1.1 include\n\nBinary instead of Textual\n\nMultiplexing - Multiple asynchronous HTTP requests over a single connection\n\nHeader compression using HPACK\n\nServer Push - Multiple responses for single request\n\nRequest Prioritization\n\nSecurity\n\n1. Binary Protocol\n\nHTTP/2 tends to address the issue of increased latency that existed in HTTP/1.x by making it a binary protocol. Being a binary protocol, it easier to parse but unlike HTTP/1.x it is no longer readable by the human eye. The major building blocks of HTTP/2 are Frames and Streams\n\nFrames and Streams\n\nHTTP messages are now composed of one or more frames. There is a HEADERS frame for the meta data and DATA frame for the payload and there exist several other types of frames (HEADERS, DATA, RST_STREAM, SETTINGS, PRIORITY etc) that you can check through the HTTP/2 specs.\n\nEvery HTTP/2 request and response is given a unique stream ID and it is divided into frames. Frames are nothing but binary pieces of data. A collection of frames is called a Stream. Each frame has a stream id that identifies the stream to which it belongs and each frame has a common header. Also, apart from stream ID being unique, it is worth mentioning that, any request initiated by client uses odd numbers and the response from server has even numbers stream IDs.\n\nApart from the HEADERS and DATA, another frame type that I think worth mentioning here is RST_STREAM which is a special frame type that is used to abort some stream i.e. client may send this frame to let the server know that I don't need this stream anymore. In HTTP/1.1 the only way to make the server stop sending the response to client was closing the connection which resulted in increased latency because a new connection had to be opened for any consecutive requests. While in HTTP/2, client can use RST_STREAM and stop receiving a specific stream while the connection will still be open and the other streams will still be in play.\n\n2. Multiplexing\n\nSince HTTP/2 is now a binary protocol and as I said above that it uses frames and streams for requests and responses, once a TCP connection is opened, all the streams are sent asynchronously through the same connection without opening any additional connections. And in turn, the server responds in the same asynchronous way i.e. the response has no order and the client uses the assigned stream id to identify the stream to which a specific packet belongs. This also solves the head-of-line blocking issue that existed in HTTP/1.x i.e. the client will not have to wait for the request that is taking time and other requests will still be getting processed.\n\n3. Header Compression\n\nIt was part of a separate RFC which was specifically aimed at optimizing the sent headers. The essence of it is that when we are constantly accessing the server from a same client there is alot of redundant data that we are sending in the headers over and over, and sometimes there might be cookies increasing the headers size which results in bandwidth usage and increased latency. To overcome this, HTTP/2 introduced header compression.\n\nUnlike request and response, headers are not compressed in gzip or compress etc formats but there is a different mechanism in place for header compression which is literal values are encoded using Huffman code and a headers table is maintained by the client and server and both the client and server omit any repetitive headers (e.g. user agent etc) in the subsequent requests and reference them using the headers table maintained by both.\n\nWhile we are talking headers, let me add here that the headers are still the same as in HTTP/1.1, except for the addition of some pseudo headers i.e. :method , :scheme , :host and :path\n\n4. Server Push\n\nServer push is another tremendous feature of HTTP/2 where the server, knowing that the client is going to ask for a certain resource, can push it to the client without even client asking for it. For example, let's say a browser loads a web page, it parses the whole page to find out the remote content that it has to load from the server and then sends consequent requests to the server to get that content.\n\nServer push allows the server to decrease the roundtrips by pushing the data that it knows that client is going to demand. How it is done is, server sends a special frame called PUSH_PROMISE notifying the client that, \"Hey, I am about to send this resource to you! Do not ask me for it.\" The PUSH_PROMISE frame is associated with the stream that caused the push to happen and it contains the promised stream ID i.e. the stream on which the server will send the resource to be pushed.\n\n5. Request Prioritization\n\nA client can assign a priority to a stream by including the prioritization information in the HEADERS frame by which a stream is opened. At any other time, client can send a PRIORITY frame to change the priority of a stream.\n\nWithout any priority information, server processes the requests asynchronously i.e. without any order. If there is priority assigned to a stream, then based on this prioritization information, server decides how much of the resources need to be given to process which request.\n\n6. Security\n\nThere was extensive discussion on whether security (through TLS) should be made mandatory for HTTP/2 or not. In the end, it was decided not to make it mandatory. However, most vendors stated that they will only support HTTP/2 when it is used over TLS. So, although HTTP/2 doesn't require encryption by specs but it has kind of become mandatory by default anyway. With that out of the way, HTTP/2 when implemented over TLS does impose some requirementsi.e. TLS version 1.2 or higher must be used, there must be a certain level of minimum keysizes, ephemeral keys are required etc.\n\nHTTP/3 - 2022\n\nHTTP/3 is the next version of HTTP. HTTP/3 is a QUIC based protocol. QUIC is a transport layer protocol which is built on top of UDP and is designed to be a replacement for TCP. It is a multiplexed, secure, stream-based protocol which is designed to reduce latency and improve performance. It is a successor to TCP and HTTP/2.\n\nQUIC is a multiplexed, secure, stream-based protocol which is designed to reduce latency and improve performance. It is a successor to TCP and HTTP/2.\n\n1. Multiplexing\n\nQUIC is a multiplexed protocol which means that multiple streams can be sent over a single connection. This is similar to HTTP/2 where multiple streams can be sent over a single connection. However, unlike HTTP/2, QUIC is not limited to HTTP. It can be used for any application that requires reliable, ordered, and loss-tolerant delivery of streams of data.\n\n2. Stream-based\n\nQUIC is a stream-based protocol which means that data is sent in the form of streams. Each stream is identified by a unique stream ID. QUIC uses a single stream to send data in both directions. This is similar to HTTP/2 where each stream is identified by a unique stream ID and each stream is bi-directional.\n\n3. Unreliable Datagram\n\nQUIC uses unreliable datagrams to send data. This means that QUIC does not guarantee that the data will be delivered to the receiver. However, QUIC does guarantee that the data will be delivered in the same order in which it was sent. This is similar to UDP where data is sent in the form of datagrams and the datagrams are not guaranteed to be delivered to the receiver.\n\n4. Connection Migration\n\nQUIC supports connection migration which means that a QUIC connection can be migrated from one IP address to another IP address. This is similar to TCP where a TCP connection can be migrated from one IP address to another IP address.\n\n5. Loss Recovery\n\nQUIC uses loss recovery to recover from packet loss. QUIC uses a combination of congestion control and loss recovery to recover from packet loss. This is similar to TCP where TCP uses a combination of congestion control and loss recovery to recover from packet loss.\n\n6. Congestion Control\n\nQUIC uses congestion control to control the rate at which data is sent over the network. QUIC uses a combination of congestion control and loss recovery to recover from packet loss. This is similar to TCP where TCP uses a combination of congestion control and loss recovery to recover from packet loss.\n\n7. Handshake\n\nQUIC uses a handshake to establish a secure connection between the client and the server. QUIC uses TLS 1.3 to establish a secure connection between the client and the server. This is similar to HTTP/2 where TLS 1.2 is used to establish a secure connection between the client and the server.\n\n8. Header Compression\n\nQUIC uses header compression to reduce the size of the headers. QUIC uses HPACK to compress the headers. This is similar to HTTP/2 where HPACK is used to compress the headers.\n\n9. Security\n\nQUIC uses TLS 1.3 to establish a secure connection between the client and the server. This is similar to HTTP/2 where TLS 1.2 is used to establish a secure connection between the client and the server.\n\nConclusion\n\nIn this article, we have discussed HTTP/1.1, HTTP/2, and HTTP/3. We have also discussed the differences between HTTP/1.1 and HTTP/2 and HTTP/2 and HTTP/3. I hope you found this article helpful. If you have any questions, please feel free to reach out to me.",
    "article_title": "Everything you need to know about HTTP",
    "article_authors": null,
    "article_image": "https://cs.fyi/manifest/icon32.png",
    "article_pub_date": null,
    "article_url": "https://cs.fyi/guide/http-in-depth",
    "article_html": "<div><p>HTTP is the protocol that every web developer should know, as it powers the entire web. Knowing HTTP can certainly help you develop better applications.</p><p>In this article, I will discuss what HTTP is, how it came to be, where it stands today, and how we got here</p><h2 id=\"what-is-http\">What is HTTP?</h2><p>First things first, what is HTTP? HTTP is a TCP/IP-based application layer communication protocol that standardizes how clients and servers communicate with each other. It defines how content is requested and transmitted across the internet. By application layer protocol, I mean that it's simply an abstraction layer that standardizes how hosts (clients and servers) communicate. HTTP itself depends on TCP/IP to get requests and responses between the client and server. By default, TCP port 80 is used, but other ports can also be used. HTTPS, however, uses port 443.</p><h2 id=\"http09---the-one-liner-1991\">HTTP/0.9 - The One Liner (1991)</h2><p>The first documented version of HTTP was <a href=\"https://www.w3.org/Protocols/HTTP/AsImplemented.html\" rel=\"nofollow\" target=\"_blank\">HTTP/0.9</a> which was put forward in 1991. It was the simplest protocol ever; having a single method called GET. If a client had to access some webpage on the server, it would have made the simple request like below</p><pre class=\"astro-code\"><code><p class=\"line\"><p>GET /index.html</p></p></code></pre><p>And the response from server would have looked as follows</p><pre class=\"astro-code\"><code><p class=\"line\"><p>(response body)</p></p>\n<p class=\"line\"><p>(connection closed)</p></p></code></pre><p>That is, the server would get the request, reply with the HTML in response and as soon as the content has been transferred, the connection will be closed. There were</p><ul><li>No headers</li><li><code>GET</code> was the only allowed method</li><li>Response had to be HTML</li></ul><p>As you can see, the protocol really had nothing more than being a stepping stone for what was to come.</p><h2 id=\"http10---1996\">HTTP/1.0 - 1996</h2><p>In 1996, the next version of HTTP i.e. HTTP/1.0 evolved that vastly improved over the original version.</p><p>Unlike HTTP/0.9 which was only designed for HTML response, HTTP/1.0 could now deal with other response formats i.e. images, video files, plain text or any other content type as well. It added more methods (i.e. POST and HEAD), request/response formats got changed, HTTP headers got added to both the request and responses, status codes were added to identify the response, character set support was introduced, multi-part types, authorization, caching, content encoding and more was included.</p><p>Here is how a sample HTTP/1.0 request and response might have looked like:</p><pre class=\"astro-code\"><code><p class=\"line\"><p>GET / HTTP/1.0</p></p>\n<p class=\"line\"><p>Host: cs.fyi</p></p>\n<p class=\"line\"><p>User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5)</p></p>\n<p class=\"line\"><p>Accept: */*</p></p></code></pre><p>As you can see, alongside the request, client has also sent it's personal information, required response type etc. While in HTTP/0.9 client could never send such information because there were no headers.</p><p>Example response to the request above may have looked like below</p><pre class=\"astro-code\"><code><p class=\"line\"><p>HTTP/1.0 200 OK</p><p> </p></p>\n<p class=\"line\"><p>Content-Type</p><p>:</p><p> </p><p>text/plain</p></p>\n<p class=\"line\"><p>Content-Length</p><p>:</p><p> </p><p>137582</p></p>\n<p class=\"line\"><p>Expires</p><p>:</p><p> </p><p>Thu, 05 Dec 1997 16:00:00 GMT</p></p>\n<p class=\"line\"><p>Last-Modified</p><p>:</p><p> </p><p>Wed, 5 August 1996 15:55:28 GMT</p></p>\n<p class=\"line\"><p>Server</p><p>:</p><p> </p><p>Apache 0.84</p></p>\n<p class=\"line\"></p>\n<p class=\"line\"><p>(response body)</p></p>\n<p class=\"line\"><p>(connection closed)</p></p></code></pre><p>In the very beginning of the response there is HTTP/1.0 (HTTP followed by the version number), then there is the status code 200 followed by the reason phrase (or description of the status code, if you will).</p><p>In this newer version, request and response headers were still kept as ASCII encoded, but the response body could have been of any type i.e. image, video, HTML, plain text or any other content type. So, now that server could send any content type to the client; not so long after the introduction, the term \"Hyper Text\" in HTTP became misnomer. HMTP or Hypermedia transfer protocol might have made more sense but, I guess, we are stuck with the name for life.</p><p>One of the major drawbacks of HTTP/1.0 were you couldn't have multiple requests per connection. That is, whenever a client will need something from the server, it will have to open a new TCP connection and after that single request has been fulfilled, connection will be closed. And for any next requirement, it will have to be on a new connection. Why is it bad? Well, let's assume that you visit a webpage having 10 images, 5 stylesheets and 5 javascript files, totalling to 20 items that needs to fetched when request to that webpage is made. Since the server closes the connection as soon as the request has been fulfilled, there will be a series of 20 separate connections where each of the items will be served one by one on their separate connections. This large number of connections results in a serious performance hit as requiring a new TCP connection imposes a significant performance penalty because of three-way handshake followed by slow-start.</p><h3 id=\"three-way-handshake\">Three-way Handshake</h3><p>Three-way handshake in it's simples form is that all the TCP connections begin with a three-way handshake in which the client and the server share a series of packets before starting to share the application data.</p><ul><li>SYN - Client picks up a random number, let's say x, and sends it to the server.</li><li>SYN ACK - Server acknowledges the request by sending an ACK packet back to the client which is made up of a random number, let's say y picked up by server and the number x+1 where x is the number that was sent by the client</li><li>ACK - Client increments the number y received from the server and sends an ACK packet back with the number y+1</li></ul><p>Once the three-way handshake is completed, the data sharing between the client and server may begin. It should be noted that the client may start sending the application data as soon as it dispatches the last ACK packet but the server will still have to wait for the ACK packet to be recieved in order to fulfill the request.</p><p><img alt=\"3 way handshake\" src=\"https://i.imgur.com/ohZthqB.png\"></p><p>However, some implementations of HTTP/1.0 tried to overcome this issue by introducing a new header called Connection: keep-alive which was meant to tell the server \"Hey server, do not close this connection, I need it again\". But still, it wasn't that widely supported and the problem still persisted.</p><p>Apart from being connectionless, HTTP also is a stateless protocol i.e. server doesn't maintain the information about the client and so each of the requests has to have the information necessary for the server to fulfill the request on it's own without any association with any old requests. And so this adds fuel to the fire i.e. apart from the large number of connections that the client has to open, it also has to send some redundant data on the wire causing increased bandwidth usage.</p><h2 id=\"http11---1997\">HTTP/1.1 - 1997</h2><p>After merely 3 years of HTTP/1.0, the next version i.e. HTTP/1.1 was released in 1999; which made alot of improvements over it's predecessor. The major improvements over HTTP/1.0 included</p><ul><li><p>New HTTP methods were added, which introduced PUT, PATCH, OPTIONS, DELETE</p></li><li><p>Hostname Identification In HTTP/1.0 Host header wasn't required but HTTP/1.1 made it required.</p></li><li><p>Persistent Connections As discussed above, in HTTP/1.0 there was only one request per connection and the connection was closed as soon as the request was fulfilled which resulted in accute performance hit and latency problems. HTTP/1.1 introduced the persistent connections i.e. connections weren't closed by default and were kept open which allowed multiple sequential requests. To close the connections, the header Connection: close had to be available on the request. Clients usually send this header in the last request to safely close the connection.</p></li><li><p>Pipelining It also introduced the support for pipelining, where the client could send multiple requests to the server without waiting for the response from server on the same connection and server had to send the response in the same sequence in which requests were received. But how does the client know that this is the point where first response download completes and the content for next response starts, you may ask! Well, to solve this, there must be Content-Length header present which clients can use to identify where the response ends and it can start waiting for the next response.</p></li></ul><blockquote><p>It should be noted that in order to benefit from persistent connections or pipelining, Content-Length header must be available on the response, because this would let the client know when the transmission completes and it can send the next request (in normal sequential way of sending requests) or start waiting for the the next response (when pipelining is enabled).</p><p>But there was still an issue with this approach. And that is, what if the data is dynamic and server cannot find the content length before hand? Well in that case, you really can't benefit from persistent connections, could you?! In order to solve this HTTP/1.1 introduced chunked encoding. In such cases server may omit content-Length in favor of chunked encoding (more to it in a moment). However, if none of them are available, then the connection must be closed at the end of request.</p></blockquote><ul><li><p>Chunked Transfers In case of dynamic content, when the server cannot really find out the Content-Length when the transmission starts, it may start sending the content in pieces (chunk by chunk) and add the Content-Length for each chunk when it is sent. And when all of the chunks are sent i.e. whole transmission has completed, it sends an empty chunk i.e. the one with Content-Length set to zero in order to identify the client that transmission has completed. In order to notify the client about the chunked transfer, server includes the header Transfer-Encoding: chunked</p></li><li><p>Unlike HTTP/1.0 which had Basic authentication only, HTTP/1.1 included digest and proxy authentication</p></li><li><p>Caching</p></li><li><p>Byte Ranges</p></li><li><p>Character sets</p></li><li><p>Language negotiation</p></li><li><p>Client cookies</p></li><li><p>Enhanced compression support</p></li><li><p>New status codes</p></li><li><p>..and more</p></li></ul><p>I am not going to dwell about all the HTTP/1.1 features in this post as it is a topic in itself and you can already find a lot about it. The one such document that I would recommend you to read is <a href=\"http://www.ra.ethz.ch/cdstore/www8/data/2136/pdf/pd1.pdf\" rel=\"nofollow\" target=\"_blank\">Key differences</a> between HTTP/1.0 and HTTP/1.1 and here is the link to <a href=\"https://tools.ietf.org/html/rfc2616\" rel=\"nofollow\" target=\"_blank\">original RFC</a> for the overachievers.</p><p>HTTP/1.1 was introduced in 1999 and it had been a standard for many years. Although, it improved alot over it's predecessor; with the web changing everyday, it started to show it's age. Loading a web page these days is more resource-intensive than it ever was. A simple webpage these days has to open more than 30 connections. Well HTTP/1.1 has persistent connections, then why so many connections? you say! The reason is, in HTTP/1.1 it can only have one outstanding connection at any moment of time. HTTP/1.1 tried to fix this by introducing pipelining but it didn't completely address the issue because of the head-of-line blocking where a slow or heavy request may block the requests behind and once a request gets stuck in a pipeline, it will have to wait for the next requests to be fulfilled. To overcome these shortcomings of HTTP/1.1, the developers started implementing the workarounds, for example use of spritesheets, encoded images in CSS, single humungous CSS/Javascript files, domain sharding etc.</p><h2 id=\"spdy---2009\">SPDY - 2009</h2><p>Google went ahead and started experimenting with alternative protocols to make the web faster and improving web security while reducing the latency of web pages. In 2009, they announced SPDY.</p><blockquote><p>SPDY is a trademark of Google and isn't an acronym.</p></blockquote><p>It was seen that if we keep increasing the bandwidth, the network performance increases in the beginning but a point comes when there is not much of a performance gain. But if you do the same with latency i.e. if we keep dropping the latency, there is a constant performance gain. This was the core idea for performance gain behind SPDY, decrease the latency to increase the network performance.</p><blockquote><p>For those who don't know the difference, latency is the delay i.e. how long it takes for data to travel between the source and destination (measured in milliseconds) and bandwidth is the amount of data transfered per second (bits per second).</p></blockquote><p>The features of SPDY included, multiplexing, compression, prioritization, security etc. I am not going to get into the details of SPDY, as you will get the idea when we get into the nitty gritty of HTTP/2 in the next section as I said HTTP/2 is mostly inspired from SPDY.</p><p>SPDY didn't really try to replace HTTP; it was a translation layer over HTTP which existed at the application layer and modified the request before sending it over to the wire. It started to become a defacto standards and majority of browsers started implementing it.</p><p>In 2015, at Google, they didn't want to have two competing standards and so they decided to merge it into HTTP while giving birth to HTTP/2 and deprecating SPDY.</p><h2 id=\"http2---2015\">HTTP/2 - 2015</h2><p>By now, you must be convinced that why we needed another revision of the HTTP protocol. HTTP/2 was designed for low latency transport of content. The key features or differences from the old version of HTTP/1.1 include</p><ul><li>Binary instead of Textual</li><li>Multiplexing - Multiple asynchronous HTTP requests over a single connection</li><li>Header compression using HPACK</li><li>Server Push - Multiple responses for single request</li><li>Request Prioritization</li><li>Security</li></ul><p><img alt=\"\" src=\"https://i.imgur.com/X1BT5eX.png\"></p><h3 id=\"1-binary-protocol\">1. Binary Protocol</h3><p>HTTP/2 tends to address the issue of increased latency that existed in HTTP/1.x by making it a binary protocol. Being a binary protocol, it easier to parse but unlike HTTP/1.x it is no longer readable by the human eye. The major building blocks of HTTP/2 are Frames and Streams</p><h4 id=\"frames-and-streams\">Frames and Streams</h4><p>HTTP messages are now composed of one or more frames. There is a HEADERS frame for the meta data and DATA frame for the payload and there exist several other types of frames (HEADERS, DATA, RST_STREAM, SETTINGS, PRIORITY etc) that you can check through <a href=\"https://http2.github.io/http2-spec/#FrameTypes\" rel=\"nofollow\" target=\"_blank\">the HTTP/2 specs</a>.</p><p>Every HTTP/2 request and response is given a unique stream ID and it is divided into frames. Frames are nothing but binary pieces of data. A collection of frames is called a Stream. Each frame has a stream id that identifies the stream to which it belongs and each frame has a common header. Also, apart from stream ID being unique, it is worth mentioning that, any request initiated by client uses odd numbers and the response from server has even numbers stream IDs.</p><p>Apart from the HEADERS and DATA, another frame type that I think worth mentioning here is RST_STREAM which is a special frame type that is used to abort some stream i.e. client may send this frame to let the server know that I don't need this stream anymore. In HTTP/1.1 the only way to make the server stop sending the response to client was closing the connection which resulted in increased latency because a new connection had to be opened for any consecutive requests. While in HTTP/2, client can use RST_STREAM and stop receiving a specific stream while the connection will still be open and the other streams will still be in play.</p><h3 id=\"2-multiplexing\">2. Multiplexing</h3><p>Since HTTP/2 is now a binary protocol and as I said above that it uses frames and streams for requests and responses, once a TCP connection is opened, all the streams are sent asynchronously through the same connection without opening any additional connections. And in turn, the server responds in the same asynchronous way i.e. the response has no order and the client uses the assigned stream id to identify the stream to which a specific packet belongs. This also solves the head-of-line blocking issue that existed in HTTP/1.x i.e. the client will not have to wait for the request that is taking time and other requests will still be getting processed.</p><h3 id=\"3-header-compression\">3. Header Compression</h3><p>It was part of a separate RFC which was specifically aimed at optimizing the sent headers. The essence of it is that when we are constantly accessing the server from a same client there is alot of redundant data that we are sending in the headers over and over, and sometimes there might be cookies increasing the headers size which results in bandwidth usage and increased latency. To overcome this, HTTP/2 introduced header compression.</p><p>Unlike request and response, headers are not compressed in gzip or compress etc formats but there is a different mechanism in place for header compression which is literal values are encoded using Huffman code and a headers table is maintained by the client and server and both the client and server omit any repetitive headers (e.g. user agent etc) in the subsequent requests and reference them using the headers table maintained by both.</p><p>While we are talking headers, let me add here that the headers are still the same as in HTTP/1.1, except for the addition of some pseudo headers i.e. <code>:method</code>, <code>:scheme</code>, <code>:host</code> and <code>:path</code></p><h3 id=\"4-server-push\">4. Server Push</h3><p>Server push is another tremendous feature of HTTP/2 where the server, knowing that the client is going to ask for a certain resource, can push it to the client without even client asking for it. For example, let's say a browser loads a web page, it parses the whole page to find out the remote content that it has to load from the server and then sends consequent requests to the server to get that content.</p><p>Server push allows the server to decrease the roundtrips by pushing the data that it knows that client is going to demand. How it is done is, server sends a special frame called PUSH_PROMISE notifying the client that, \"Hey, I am about to send this resource to you! Do not ask me for it.\" The PUSH_PROMISE frame is associated with the stream that caused the push to happen and it contains the promised stream ID i.e. the stream on which the server will send the resource to be pushed.</p><h3 id=\"5-request-prioritization\">5. Request Prioritization</h3><p>A client can assign a priority to a stream by including the prioritization information in the HEADERS frame by which a stream is opened. At any other time, client can send a PRIORITY frame to change the priority of a stream.</p><p>Without any priority information, server processes the requests asynchronously i.e. without any order. If there is priority assigned to a stream, then based on this prioritization information, server decides how much of the resources need to be given to process which request.</p><h3 id=\"6-security\">6. Security</h3><p>There was extensive discussion on whether security (through TLS) should be made mandatory for HTTP/2 or not. In the end, it was decided not to make it mandatory. However, most vendors stated that they will only support HTTP/2 when it is used over TLS. So, although HTTP/2 doesn't require encryption by specs but it has kind of become mandatory by default anyway. With that out of the way, HTTP/2 when implemented over TLS does impose some requirementsi.e. TLS version 1.2 or higher must be used, there must be a certain level of minimum keysizes, ephemeral keys are required etc.</p><h2 id=\"http3---2022\">HTTP/3 - 2022</h2><p>HTTP/3 is the next version of HTTP. HTTP/3 is a QUIC based protocol. QUIC is a transport layer protocol which is built on top of UDP and is designed to be a replacement for TCP. It is a multiplexed, secure, stream-based protocol which is designed to reduce latency and improve performance. It is a successor to TCP and HTTP/2.</p><p>QUIC is a multiplexed, secure, stream-based protocol which is designed to reduce latency and improve performance. It is a successor to TCP and HTTP/2.</p><h3 id=\"1-multiplexing\">1. Multiplexing</h3><p>QUIC is a multiplexed protocol which means that multiple streams can be sent over a single connection. This is similar to HTTP/2 where multiple streams can be sent over a single connection. However, unlike HTTP/2, QUIC is not limited to HTTP. It can be used for any application that requires reliable, ordered, and loss-tolerant delivery of streams of data.</p><h3 id=\"2-stream-based\">2. Stream-based</h3><p>QUIC is a stream-based protocol which means that data is sent in the form of streams. Each stream is identified by a unique stream ID. QUIC uses a single stream to send data in both directions. This is similar to HTTP/2 where each stream is identified by a unique stream ID and each stream is bi-directional.</p><h3 id=\"3-unreliable-datagram\">3. Unreliable Datagram</h3><p>QUIC uses unreliable datagrams to send data. This means that QUIC does not guarantee that the data will be delivered to the receiver. However, QUIC does guarantee that the data will be delivered in the same order in which it was sent. This is similar to UDP where data is sent in the form of datagrams and the datagrams are not guaranteed to be delivered to the receiver.</p><h4 id=\"4-connection-migration\">4. Connection Migration</h4><p>QUIC supports connection migration which means that a QUIC connection can be migrated from one IP address to another IP address. This is similar to TCP where a TCP connection can be migrated from one IP address to another IP address.</p><h4 id=\"5-loss-recovery\">5. Loss Recovery</h4><p>QUIC uses loss recovery to recover from packet loss. QUIC uses a combination of congestion control and loss recovery to recover from packet loss. This is similar to TCP where TCP uses a combination of congestion control and loss recovery to recover from packet loss.</p><h4 id=\"6-congestion-control\">6. Congestion Control</h4><p>QUIC uses congestion control to control the rate at which data is sent over the network. QUIC uses a combination of congestion control and loss recovery to recover from packet loss. This is similar to TCP where TCP uses a combination of congestion control and loss recovery to recover from packet loss.</p><h4 id=\"7-handshake\">7. Handshake</h4><p>QUIC uses a handshake to establish a secure connection between the client and the server. QUIC uses TLS 1.3 to establish a secure connection between the client and the server. This is similar to HTTP/2 where TLS 1.2 is used to establish a secure connection between the client and the server.</p><h4 id=\"8-header-compression\">8. Header Compression</h4><p>QUIC uses header compression to reduce the size of the headers. QUIC uses HPACK to compress the headers. This is similar to HTTP/2 where HPACK is used to compress the headers.</p><h4 id=\"9-security\">9. Security</h4><p>QUIC uses TLS 1.3 to establish a secure connection between the client and the server. This is similar to HTTP/2 where TLS 1.2 is used to establish a secure connection between the client and the server.</p><h2 id=\"conclusion\">Conclusion</h2><p>In this article, we have discussed HTTP/1.1, HTTP/2, and HTTP/3. We have also discussed the differences between HTTP/1.1 and HTTP/2 and HTTP/2 and HTTP/3. I hope you found this article helpful. If you have any questions, please feel free to reach out to me.</p></div>",
    "article_abstract": null
  }